\documentclass[sigconf,natbib=true, review=true]{acmart} %
\usepackage{graphicx} % Required for inserting images
\usepackage[show]{chato-notes}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{subfig}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{placeins}
\usepackage{multirow}
\usepackage{listings}
\usepackage[skins]{tcolorbox}



\usepackage{marginnote}
%\newcommand{\pageenlarge}[1]{\marginnote{#1}\enlargethispage{#1\baselineskip}}
\newcommand{\pageenlarge}[1]{\marginnote{}\enlargethispage{#1\baselineskip}}
%\newcommand{\pageenlarge}[1]
%\newcommand{\sasha}[1]{\textcolor{ACMred}{#1}}
%\newcommand{\sasha}[1]{\textcolor[HTML]{FF0000}{#1}}
%\newcommand{\craig}[1]{\textcolor{red}{#1}}
%\newcommand{\nt}[1]{\textcolor{blue}{#1}}

\newcommand{\sasha}[1]{\textcolor[HTML]{000000}{#1}}
\newcommand{\rsasha}[1]{\textcolor[HTML]{FF0000}{#1}}
\newcommand{\craig}[1]{\textcolor{black}{#1}}
\newcommand{\nt}[1]{\textcolor{black}{#1}}
\newcommand{\pluseq}{\mathrel{+}=}


\newcommand{\argmax}{\operatornamewithlimits{\sf argmax}}



%\title{Pruning sub-item representations for large-catalogue recommender models}
\title{Efficient Inference of Sub-Item Id-based Sequential Recommendation Models with Millions of Items}
\copyrightyear{2024} 
\acmYear{2024} 

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317</concept_id>
       <concept_desc>Information systems~Information retrieval</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Information retrieval}

%
% Keywords. The author(s) should pick words that accurately describe
% the work being presented. Separate the keywords with commas.
\keywords{Recommender Systems, Inferenece}


\setcopyright{acmlicensed}\acmConference[RecSys '24]{18th ACM Conference on Recommender Systems
}{October 14--18 2024}{Bari, Italy}
\acmBooktitle{18th ACM Conference on Recommender Systems, October 14--18 2024, Bari, Italy}


\author{Anonymous Author(s)}
\affiliation{%
  \institution{Organisation} \country{A Country}}
\email{email@example.com}



% \author{Aleksandr V. Petrov}
% \affiliation{%
%   \institution{University of Glasgow} \country{United Kingdom}}

% \email{a.petrov.1@research.gla.ac.uk}

% \author{Craig Macdonald}
% \affiliation{%
%   \institution{University of Glasgow} \country{United Kingdom}}
% \email{craig.macdonald@glasgow.ac.uk}



% \author{Nicola Tonellotto}
% \affiliation{%
%   \institution{University of Pisa} \country{Italy}}
% \email{nicola.tonellotto@unipi.it}
\begin{document}

\begin{abstract}
Transformer-based recommender systems, such as BERT4Rec or SASRec, achieve state-of-the-art results in sequential recommendation. However, these models are rarely used in production due to limited catalogue size: in production, it is common to have millions of items in the catalogue, while scaling transformers beyond a few thousand items is problematic due to large GPU memory requirements, inefficient training and slow inference. Several recent papers addressed inefficient training and large memory requirements, but slow inference with large catalogues remains problematic. In particular, RecJPQ is a state-of-the-art method of reducing the model's memory consumption; RecJPQ achieves compression by decomposing item IDs into a small number of shared sub-item IDs. Despite reporting the reduction of memory consumption by a factor of up to 50x, the original RecJPQ paper did not report inference efficiency improvements over the baseline transformer-based models. To address inference efficiency, we propose to apply PQTopK, an efficient item-scoring algorithm applied on top of RecJPQ-based models and find that we can increase the efficiency of these models by a large factor. In particular, we speed up RecJPQ-enhanced SASRec by a factor of 4.5$\times$ compared to the original SASRec's inference method and by the factor of  1.56$\times$ compared to the method implemented in RecJPQ code on a large-scale Gowalla dataset with more than million items. Using simulated data, we show that PQTopK remains efficient with catalogues of up to tens of millions of items, removing one of the last obstacles to using transformer-based models in production environments with large catalogues.
\end{abstract}

\maketitle

\section{Introduction and Related work}
The goal of Sequential Recommender models is to predict the next item in a sequence of user-item interactions. The best models for this task, such as are based on adaptations of Transformer~\cite{Transformer}-based models~\cite{SASRec, BERT4Rec, Bert4RecRepro, petrovGSASRecReducingOverconfidence2023}. 
Originally, the Transformer~\cite{Transformer} architecture had been developed for the language processing domain, but recommender systems adapt the architecture by using items instead of tokens. 

Despite achieving state-of-the-art results on the "toy" academic datasets, it is challenging to use these models in a production environment due to the scalability issues: the number of items in large-scale recommender systems, such as product recommendations in Amazon, can reach hundreds of millions, which is much larger than tens of thousands of tokens in the dictionaries of language models.  A large catalogue causes a number of problems in transformer models, such as large GPU memory requirement to store the item embeddings, large computational resources required to model training, and slow inference in production. Several works have recently addressed the memory consumption issues~\cite{xiaEfficientOnDeviceSessionBased2023, petrovRecJPQTrainingLargeCatalogue2024} and inefficient training~\cite{klenitskiyTurningDrossGold2023, petrovGSASRecReducingOverconfidence2023, petrovRSSEffectiveEfficient2023}; however, efficient model inference remains an open question, which we aim to address in this paper. Additionally, the large catalogue size is usually associated with a large user base, meaning that the trained model has to be inferred frequently and in parallel for many users. Therefore, to reduce the maintenance costs of a model, it is important to avoid the use of expensive GPU accelerators during the inference stage. 

The main cause of slow inference of transformer-based models arises not from the transformer backbone model itself but from the computing scores for individual items. Indeed, the inference time of the backbone transformer is constant with respect to the number of items; however, computing item scores has linear complexity with respect to the number of items. Hence, to speed up inference, there are three options: (i) reduce the number of scored items, (ii) reduce the number of operations per item, and (iii) efficiently parallelise computations. 


In the first category fall the approximate nearest neighbour methods, such as FAISS~\cite{FAISS} or Annoy~\cite{SpotifyAnnoy2024}. While these methods can be practical in some cases, there are two problems: (i) these methods are \emph{unsafe}, meaning that the result return from the ANN index may omit some candidates that would have been scored high by the model and (ii) they require item embeddings to be present in the first place in order to build the index, and training item embeddings for all items in large catalogue case maybe not feasible in the first place~\cite{petrovRecJPQTrainingLargeCatalogue2024}.

Therefore, this paper focuses on reducing the number of operations per item and parallelising the computations. In particular, we build upon RecJPQ~\cite{petrovRecJPQTrainingLargeCatalogue2024}, a recent state-of-the-art approach for compressing embedding tables in Transformer-based sequential recommenders. RecJPQ achieves compression by representing items using a concatenation of shared sub-ids. While achieving great results on compression (for example, on the Gowalla~\cite{g} dataset, RecJPQ achieves up to 50$\times$ compression without degrading effectiveness), the RecJPQ paper doesn't perform an analysis of the model's inference in large catalogues and only briefly mentions that it is similar to the inference time of the original non-compressed models. On the other hand, prior works that built upon similar ideas of sub-id-based recommendation, such as LightRec~\cite{lianLightRecMemorySearchEfficient2020}, show that the sub-id-based method can indeed improve model inference time. 



Therefore, in this paper, we analyse the following research hypothesis:
\begin{tcolorbox}[enhanced, drop shadow, title={Hypothesis H1}]

\label{hyp:h0}
    RecJPQ-enhanced Transformer-based recommendation models can be efficiently inferred on catalogues with (multiple) millions of items without using GPU acceleration. 
\end{tcolorbox}

The main contributions of this paper can be summarised as follows: (i) We analyse inference efficiency of RecJPQ-enhanced versions of SASRec~\cite{SASRec} and BERT4Rec~\cite{BERT4Rec} and find that it is more efficient than Matrix-Multiplication based scoring used in the original models; (ii) we show that scoring of RecJPQ-based models can be further improved by employing better parallelization and (iii) we explore the limits of RecJPQ-based inference using simulated settings with up to 1 Billion items in catalogue and show that inference remains efficient with millions of items.
Note that the main contribution of this paper is the exploration of the limits of existing methods. However, to the best of our knowledge, this is the first analysis of the inference of sub-id-based sequential models on large-scale datasets and the first demonstration of the feasibility of using these models in the large-catalogue scenario. 

The rest of the paper is organised as follows: Section~\ref{ssec:recsys:preliminarilies} covers the background material on Transformers for a large-catalogue sequential recommendation; Section~\ref{sec:pq_topk} describes PQTopK, an efficient scoring algorithm for sub-id-based recommenders; Section~\ref{sec:experiments} contains experimental evaluation of different scoring methods on real and simulated data; Section~\ref{sec:conclusion} contains concluding remarks. 


\section{Background} \label{ssec:recsys:preliminarilies}
\subsection{Transformers for Large-catalogue Sequential Recommendation}\label{sec:rec}

\looseness -1 Usually, sequential recommendation is cast as the \emph{next item prediction} task. Formally, given a chronologically ordered sequence of user-item interactions $\left\{i_1, i_2, i_3 ... i_n\right\}$, \nt{also known as their \textit{interactions history},} the goal of a recommender system is to predict the next item in the sequence, $i_{n+1}$ from the \emph{item catalogue} (the set of all possible items) $I$. The total number of items $|I|$ is the \emph{catalogue size}.

% \inote{cannot parse this sentence} \inote{Nic: the following sentence fits more in introduction or related works IMO.}
Sequential recommendation bears a resemblance to the natural language processing task of \emph{next token prediction}, as addressed by language models, such as GPT-2~\cite{gpt2}.
%In the natural language processing domain, a similar task is the \emph{next token prediction} task, which is one of the most common tasks in generative language models, such as GPT-2~\cite{gpt2}. 
Hence, researchers have adapted language models to the recommendation task by replacing token IDs (in language models) with item IDs (in recommendation models). In particular, Transformer~\cite{Transformer}-based models, such as SASRec~\cite{SASRec} (based on the Transformer Decoder architecture, similar to GPT-2~\cite{gpt2})  and BERT4Rec~\cite{BERT4Rec} (based on Transformer Encoder, similar to BERT) have demonstrated state-of-the-art results for sequential recommendation~\cite{Bert4RecRepro}. 


Typically, to generate recommendations given a history of interactions $h = \left\{i_1, i_2, i_3 ... i_n\right\}$, Transformer-based models first generate a sequence embedding $\phi \in \mathbb{R}^d$, where $d$ is the embedding dimensionality, using the Transformer model, such that $\phi =  \textsf{Transformer}(h)$.
% \begin{align}
% \phi =  \textsf{Transformer}(h)
% \end{align}
The scores for all items, $r = (r_1, \ldots, r_{|I|}) \in \mathbb{R}^{|I|}$, are then computed by multiplying the matrix of item embeddings $W \in \mathbb{R}^{|I|\times d}$, which is usually shared with the embeddings layer of the Transformer model, by the sequence embedding $\phi$\footnote{This also generalises to models that have item biases (e.g.\ BERT4Rec), if we assume that the first dimension of sequence embedding equals 1 and the first dimension of item embedding contains item bias.}:
\begin{align}
r = W\phi\label{eq:scores}
\end{align}

Finally, the model generates recommendations by selecting for $r$ the top $K$ items with the highest scores.%:
%\begin{align}
%    \text{Recommendations} = \textsf{TopK}(R, k)
%k\argmax_{i \in I} (r_i)
%\end{align}\inote{I dont follow this eqn}

Despite their effectiveness, training Transformer-based models with large item catalogues is a challenging task, as these models typically have to be trained for long time~\cite{Bert4RecRepro} and require appropriate selection of training objective~\cite{petrovRSSEffectiveEfficient2023}, negative sampling strategy and loss function~\cite{petrovGSASRecReducingOverconfidence2023,klenitskiyTurningDrossGold2023}.
%
Transformer-based models with large catalogues also require a lot of memory to store the item embeddings $W$. This problem has recently been addressed by Product Quantiantisation, which we describe in the next section. 
%
Finally, another problem with transformer-based models with large catalogues is their slow inference with large catalogues. Indeed, computing all item scores using Equation~\eqref{eq:scores} may be prohibitively expensive when the item catalogue is large: it requires $|I|\times d$ scalar multiplications and additions, and, as we noted in Section~\ref{sec:intro},  in real-world recommender systems, the catalogue size $|I|$ may reach hundreds of millions of items, making exhaustive computation using Equation~\eqref{eq:scores} impractical. Moreover, typically large-catalogue recommender systems have a large number of users as well; therefore, the model has to be used for inference very frequently, and, ideally, using only low-cost hardware, i.e., without using GPU acceleration. Therefore, real-life recommender systems rarely exhaustively score all items for all users and instead apply unsafe heuristics (i.e.\ heuristics that do not provide theoretical guarantees that all high-scored items will be returned, such as two-stage ranking). 
 
 %\pageenlarge{1}
\subsection{Sub-Id based recommendation}\label{ssec:recjpq}


\begin{figure}[tb]\hspace{-3mm}
    \vspace{-0.5\baselineskip}
\includegraphics[width=0.5\textwidth]{figures/pq_scoring_graffle.pdf}%\vspace{-.75\baselineskip}
    \caption{Reconstruction of item embeddings \sasha{and computing item scores} using Product Quantisation \sasha{with $m=3$ splits.} }\vspace{-.75\baselineskip}
    \label{fig:embedding_reconstruction}
\end{figure}


As we noted above, with large item catalogues, the item embedding matrix $W$ requires too much memory to store. Indeed, when the catalogue size reaches several million items, the item embedding matrix becomes too large to fit in a GPU's memory~\cite{petrovRecJPQTrainingLargeCatalogue2024}. To reduce its memory footprint, several recent recommendation models adopted Product Quantisation (PQ)~\cite{jegouProductQuantizationNearest2011} -- a well-studied method for vector compression. PQ is a generic method that can be applied to any \nt{set of vectors}; however, in this section, we \nt{describe} PQ as applied to item embedding. 

\looseness -1 To compress the item embedding matrix $W$, PQ associates each item id to a list of {\em sub-ids}, akin to language models breaking down words into sub-word tokens. PQ reconstructs an item's embedding by combining the sub-id embeddings assigned to it. These sub-ids are organised into {\em splits}, with each item having precisely one sub-id from each split\footnote{We use the terminology from~\cite{petrovRecJPQTrainingLargeCatalogue2024}. In the PQ literature, another popular term for sub-id embeddings is \emph{centroids}, and another popular term for splits is \emph{sub-spaces}.}. More formally, PQ first builds a \emph{codebook} $G\in\mathbb{N}^{|I|\times m}$ that maps an item id $i$ to its associated $m$ sub-ids:
\begin{align}
    G[i] \rightarrow \left\{g_{i1}, g_{i2}, ..., g_{im}\right\} \label{eq:sub_ids_map}
\end{align}
where $g_{ij}$ is the $j$-th sub-id associated with item $i$ and $m$ is the number of splits. To achieve compression compared to storing a full item embedding matrix, $m$ is typically chosen much smaller compared to full embeddings dimensionality: $m \ll d$.
The exact algorithm for associating ids with sub-ids is specific to different implementations of PQ. For example, the original PQ implementation uses variations of K-Means clustering, Differentiable Product Quantisation proposes a method to learn the mapping~\cite{chenDifferentiableProductQuantization2020} as part of overall model training, and RecJPQ~\cite{petrovRecJPQTrainingLargeCatalogue2024} derives the codes from a truncated SVD decomposition of the user-item interactions matrix. 

%\pageenlarge{1}

For each split $k=1,\ldots,m$, PQ stores a sub-item embedding matrix $\Psi_k \in \mathbb{R}^{b\times\frac{d}{m}}$, where $b$ is the number of distinct sub-ids in each split. The $j$-th row of $\Psi_k$ denotes the sub-item embedding $\psi_{k,j} \in \mathbb{R}^{\frac{d}{m}}$ associated with the $j$-th sub-id, in the $k$-th split.
Then, PQ reconstructs the item embedding $w_i$ as a concatenation of the associated sub-id embedding: 
\begin{align}
    w_i =  \psi_{1,g_{i1}} \mathbin\Vert \psi_{2,g_{i2}}  \mathbin\Vert ... \mathbin\Vert  \psi_{m,g_{im}} \label{eq:pq:item_embedding}
\end{align}

Finally, an item score is computed as the dot product of the  sequence embedding and the constructed item embedding: 
\begin{align}
    r_i = w_i \cdot \phi\label{eq:dot_product}
\end{align}

\section{PQTopK} \label{sec:pq_topk}
\looseness -1 A straightforward use of Equation~\eqref{eq:dot_product} for item scoring in PQ-based recommendation models does not lead to any computational efficiency improvements compared to models where all item embeddings are stored explicitly.
However, PQ offers a more efficient scoring algorithm. To achieve more efficient scoring, PQ  first splits the sequence embedding $\phi \in \mathbb{R}^{d}$ into $m$ sub-embeddings $\left\{\phi_1, \phi_2 ... \phi_m\right\}$, with $\phi_{k} \in \mathbb{R}^{\frac{d}{m}}$ for $k=1, \ldots,m$, such that $\phi = \phi_1  \mathbin\Vert \phi_2  \mathbin\Vert ... \mathbin\Vert  \phi_m$.
By substituting Equation~\eqref{eq:pq:item_embedding} and the similarly decomposed sequence embedding $\phi$ into Equation~\eqref{eq:dot_product}, we can compute the final item score for item $i$ as the sum of sub-embedding dot-products: 
\begin{align}
  %r_i = w_i \cdot \phi &= (\psi_{1,g_{i1}} \mathbin\Vert \psi_{2,g_{i2}}  \mathbin\Vert ... \mathbin\Vert  \psi_{m,g_{im}}) \cdot (\phi_1  \mathbin\Vert \phi_2  \mathbin\Vert ... \mathbin\Vert  \phi_m) \nonumber \\
  %&= \sum_{k=1}^m \psi_{k,g_{ik}} \cdot \phi_k
   r_i = w_i \cdot \phi &= (\psi_{1,g_{i1}} \mathbin\Vert ... \mathbin\Vert  \psi_{m,g_{im}}) \cdot (\phi_1  \mathbin\Vert ... \mathbin\Vert  \phi_m) = \sum_{k=1}^m \psi_{k,g_{ik}} \cdot \phi_k \nonumber
\end{align}

Let $S \in \mathbb{R}^{m \times b}$ denote the \emph{sub-id score matrix}, which consists of \emph{sub-id scores} $s_{k,j}$, defined as dot products of the sub-item embedding $\psi_{k,j}$ and the sequence sub-embeddings $\phi_k$:
\begin{align}
s_{k,j} = \psi_{k,j} \cdot \phi_k\label{eq:sub_item_scores}
\end{align}

The final score of item $i$ is then can be computed as the sum of the scores of its associated sub-ids:
\begin{align}
   r_{i} = \sum_{k=1}^m s_{k,g_{ik}} \label{eq:sum_sub_scores}
\end{align}

The number of splits $m$ and the number of sub-ids per spit $b$ are usually  chosen to be relatively small, 
so that the total number of sub-id scores is much less compared to the size of the catalogue, e.g., $m\times b \ll |I|$. 
%\begin{align}
%m\times b \ll |I| \label{eq:sub_items_scores_size}
%\end{align}
%
Therefore,  we can pre-compute matrix $S$ \sasha{for a given sequence embedding} once and then reuse these scores for all items. This leads to efficiency gains compared to matrix multiplication, as scoring each item now only requires $m \ll d$ additions instead of $d$ multiplications and $d$ additions per item (Equation~\eqref{eq:scores}). The time for pre-computing sub-item scores matrix $S$ does not depend on the catalogue size $|I|$, and %according to Equation~\eqref{eq:sub_items_scores_size},
we can assume that it is negligible compared to exhaustive scoring of all items. 

%\pageenlarge{1} 
In the paper, we refer to the summation-based approach as \textit{PQTopK}; Algorithm~\ref{alg:top_k} illustrates the PQTopK in pseudo-code. \rsasha{PQTopK} allows selecting top $K$ items not only from all items but from a specific subset of items. Note that the Algorithm has two loops: the outer loop (line~\ref{alg:outer_loop}) iterates over the items in the catalogue, and the inner loop (line~\ref{alg:inner_loop}) iterates over codes associated with the item. However, as the item scores are independent of each other, both loops can be efficiently parallelised. 



\begin{algorithm}[h]
\small
\caption{PQTopK($G$, $S$, $K$, $V$).}\label{alg:top_k}
\begin{algorithmic}[1]
   \Require $G$ is the codebook (mapping: item id $\rightarrow$ sub-item ids), Eq.~\eqref{eq:sub_ids_map}
   \Require $S$ is the matrix of pre-computed sub-item scores, indexed by split and sub-item, Eq.~\eqref{eq:sub_item_scores}

   \Require $K$ is the number of results to return
%   %\Require $m$ is the number of splits
   \Require $V \subseteq I$ are the items to score; all items ($V = I$)  if not given 
   
   \State $scores \gets$ empty array of scores for all items in $V$, initialised to 0
   \For{$item\_id \in V$} \label{alg:outer_loop} \Comment{This loop can be efficiently parallelised}
        \State $score[item\_id] \gets \sum_{k=1}^{m} S[k,G[item\_id,k]] \label{alg:inner_loop} $ \Comment{Eq.~\eqref{eq:sum_sub_scores}}
   \EndFor
   \State \Return TopK($score$, $K$) \Comment{Returns a list of $\langle$ItemId, Score$\rangle$ pairs} 
\end{algorithmic}
\end{algorithm}


\section{Experiments}\label{sec:experiments}
We designed our experiments to answer two research questions: 
\begin{itemize}
    \item[RQ1] How does PQTopK inference efficiency compare to baseline item scoring methods? 
    \item[RQ2] How does PQTopK inference efficiency change when increasing the number of items in the catalogue? 
\end{itemize}

\begin{table}
\caption{Efficiency analysis of item scoring methods. mRT is the Median Running Time, measured in milliseconds.}
\resizebox{\linewidth}{!}{
    \input{main_table}
}
\end{table}


\begin{figure}
    \centering
    \resizebox{\linewidth}{!}{
        \includegraphics{figures/simulated_figure.pdf}
    }
    \caption{Efficiency of PQTopK on simulated data}
    \label{fig:enter-label}
\end{figure}

\section{Conclusion}\label{sec:conclusion}



\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{references}

\appendix
\section{Auxiliary Materials}
\subsection{RecJPQ's Scoring Algorithm}
\begin{algorithm}[h]
\small
\caption{RecJPQTopK($G$, $S$, $K$, $V$) Scoring algorithm used in RecJPQ.}\label{alg:top_k}
\begin{algorithmic}[1]
   \Require $G$ is the codebook (mapping: item id $\rightarrow$ sub-item ids), Eq.~\eqref{eq:sub_ids_map}
   \Require $S$ is the matrix of pre-computed sub-item scores, indexed by split and sub-item, Eq.~\eqref{eq:sub_item_scores}

    

   \Require $K$ is the number of results to return
   \Require $V \subseteq I$ are the items to score; all items ($V = I$)  if not given 
   
   \State $scores \gets$ empty array of scores for all items in $V$, initialised to 0
   \For{$k \in 1..m$} \Comment{Not vectorised in RecJPQ}
        \For{$item\_id \in V$} 
            \State $score[item\_id]  \pluseq S[k,G[item\_id,k]] $ 
        \EndFor
   \EndFor
   \State \Return TopK($score$, $K$) 
\end{algorithmic}
\end{algorithm}


\end{document}
